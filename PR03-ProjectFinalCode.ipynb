{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f6de454b8e379477f0b24d4aeb46b54f",
     "grade": false,
     "grade_id": "cell-ea1356fb2fa161a8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Project Final Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c13f0fb473d6c832136a348847c3d245",
     "grade": true,
     "grade_id": "cell-520b9a1998578280",
     "locked": false,
     "points": 0.1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Objective statement:\n",
    "- I will create baseline classification standards to predict whether an Android API method manipulates/returns information associated with data types defined in privacy policies by using classical sparse vector space representations of the data with standard linear and non-linear classifiers.\n",
    "- As a secondary objective I will contrast using nltk for text processing and data exploration with using the newer spaCy and gensim packages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8afa10661e34a7e5d2ac29a291d29af8",
     "grade": false,
     "grade_id": "cell-259aa4719392cc9c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Obtaining the Data\n",
    "The data required for this project may be obtained as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6e9e40aaf5be6e8697f52969f9bf1edc",
     "grade": true,
     "grade_id": "cell-5d0765799a830601",
     "locked": false,
     "points": 0.1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "24df493960920d9ea471708f06fbace8",
     "grade": false,
     "grade_id": "cell-8770d1ab57c81792",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\ChrisCrabtree\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "from collections import Counter\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import spacy\n",
    "import nltk\n",
    "\n",
    "from spacy.tokens import Doc\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.summarization.textcleaner import get_sentences\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import pos_tag, ne_chunk\n",
    "\n",
    "nltk.download('maxent_ne_chunker')\n",
    "datadir = 'proj_data'\n",
    "source_filename = 'android_semi_cleaned.csv'\n",
    "annotated_filename = 'android_cleaned_ANNOTATED_ONLY.csv'\n",
    "mappings_data_filename = 'mappings_cleaned.csv'\n",
    "source_path = os.path.join(datadir, source_filename)\n",
    "mappings_path = os.path.join(datadir, mappings_data_filename) \n",
    "annotated_path = os.path.join(datadir, annotated_filename)\n",
    "MODEL_COMPARISON_DIR = 'model_comparisons'\n",
    "nltk_tok_f= 'nltk_tok_by_sent.npy'\n",
    "spacy_tok_f= 'spacy_tok_by_sent.npy'\n",
    "tok_diffs_f= 'tok_dif.npy'\n",
    "nltk_pos_f= 'nltk_pos_by_sent.npy'\n",
    "spacy_pos_f= 'spacy_pos_by_sent.npy'\n",
    "nltk_ner_f = 'nltk_ner.npy'\n",
    "spacy_ner_f= 'spacy_ner.npy'\n",
    "parsed_docs_f = 'parsed_docs.npy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "619970a9d0e740b6cbcd0f1cbc67b906",
     "grade": false,
     "grade_id": "cell-5864d3b3e792fc82",
     "locked": true,
     "points": 1.1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def access_data(path):\n",
    "    with open(path,'r', newline='', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f, dialect='excel')\n",
    "        docs = []\n",
    "        methods = []\n",
    "        method_documents = {}\n",
    "        for i, row in enumerate(reader):\n",
    "            method = row[0]\n",
    "            doc = row[1] \n",
    "            if method in method_documents:\n",
    "                if [sent for sent in method_documents[method] if sent == doc] == []:\n",
    "                    method_documents[method].append(doc)\n",
    "            methods.append(method)\n",
    "            docs.append(doc)\n",
    "        return methods, docs, method_documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_html_remover(doc):\n",
    "    return re.sub('''</?\\w+((\\s+\\w+(\\s*=\\s*(?:\".*?\"|'.*?'|[\\^'\">\\s]+))?)+\\s*|\\s*)/?>''','', doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_cleaner(docs):\n",
    "    new_docs = []\n",
    "    for doc in docs:\n",
    "        # standardize spaces and newlines\n",
    "        new_doc = re.sub(r'\\s+', ' ', doc)\n",
    "        new_doc = re.sub(r'\\s+\\.\\s+', '. ', new_doc)\n",
    "        new_doc = my_html_remover(new_doc)\n",
    "        new_docs.append(new_doc)\n",
    "    return new_docs\n",
    "\n",
    "def get_docs(path):\n",
    "    methods, docs, docs_by_method = access_data(path)\n",
    "    docs = my_cleaner(docs)\n",
    "    return pd.DataFrame({'docs': docs, 'methods':methods}), docs_by_method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "Other cleaning such as punctuation removal and lemmatization will not be done here and will be considered later in the presentation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0a0e480232550900366ae2fc89b3dc39",
     "grade": false,
     "grade_id": "cell-141a2bb87d84a15e",
     "locked": true,
     "points": 1.2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of demonstration, I will show SpaCy's functionality in pieces, but normally text processing with SpaCy is pipelined automatically with options for minor alterations. The tag line from SpaCy is that _____ only the best tools for the job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence tokenization with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_toks_nltk(docs):\n",
    "    return [[s.strip(':. \"') for s in sent_tokenize(doc)] for doc in docs ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence tokenization with gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_toks_gensim(docs):\n",
    "    return [[s.strip(':. \"') for s in get_sentences(doc)] for doc in docs ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence tokenization comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_differences(orig_docs, nltk_docs, gensim_docs):\n",
    "    diff_cnt = 0\n",
    "    diff_orig, diff_nltk, diff_gensim = [],[],[]\n",
    "    same = []\n",
    "    for i, (orig_doc, nltk_doc, gensim_doc) in enumerate(zip(orig_docs,nltk_docs, gensim_docs)):        \n",
    "        # keep track of differences to prevent over-counting\n",
    "        diff_list=[]\n",
    "        \n",
    "        # must search sequentially to track where differences occur. Set operations would lose alignment\n",
    "        for i, (nltk_sent, gensim_sent) in enumerate(zip(nltk_doc, gensim_doc)):\n",
    "            \n",
    "            if (nltk_sent not in gensim_doc or gensim_sent not in nltk_doc) \\\n",
    "                and ([l for l in diff_list if gensim_sent in l or nltk_sent in l] == []):\n",
    "                diff_cnt += 1\n",
    "                diff_list.extend([nltk_sent,gensim_sent])\n",
    "                diff_orig.append(orig_doc)\n",
    "                diff_nltk.append(nltk_sent)\n",
    "                diff_gensim.append(gensim_sent)\n",
    "        \n",
    "            else:\n",
    "                same.append(gensim_sent)\n",
    "    diff_examples = pd.DataFrame({'Original':diff_orig,'nltk':diff_nltk,'gensim':diff_gensim})\n",
    "    \n",
    "    return diff_cnt, diff_examples, same\n",
    "        \n",
    "        \n",
    "def compare_sent_tokenization(docs):\n",
    "    \n",
    "    nltk_docs_tokenized = sent_toks_nltk(docs)\n",
    "    gensim_docs_tokenized = sent_toks_gensim(docs)\n",
    "    diff_cnt, diff_examples, _ = get_differences(docs, nltk_docs_tokenized, gensim_docs_tokenized)\n",
    "    num_nltk_sents = sum([len(d) for d in nltk_docs_tokenized])\n",
    "    num_gensim_sents = sum([len(d) for d in gensim_docs_tokenized])\n",
    "    avg_diff_len_nltk = sum([len(d.split()) for d in diff_examples['nltk']])/diff_cnt\n",
    "    avg_diff_len_gensim = sum([len(d.split()) for d in diff_examples['gensim']])/diff_cnt\n",
    "    diff_sent_lens = [(len(doc1),len(doc2)) for doc1,doc2 in zip(nltk_docs_tokenized,gensim_docs_tokenized)\\\n",
    "                                                if len(doc1) != len(doc2) ]\n",
    "    frac_nltk_greater = len(list(filter(lambda x:x[0]>x[1], diff_sent_lens)))/len(diff_sent_lens)\n",
    "    avg_diff_sents_nltk = sum([len(doc) for doc in nltk_docs_tokenized])/len(nltk_docs_tokenized)\n",
    "    avg_diff_sents_gensim = sum([len(doc) for doc in gensim_docs_tokenized])/len(gensim_docs_tokenized)\n",
    "    print(f\"Total number of documents: {len(docs):,}\")\n",
    "    print(f\"Number of differences: {diff_cnt:,}\")\n",
    "    print(f\"{'NLTK':>65}{'Gensim':>20}\")\n",
    "    print(f\"{'Number of senteces:':>45}{num_nltk_sents:>20,}{num_gensim_sents:>20,}\")\n",
    "    print(f\"{'Avg. #tokens of differences:':>45}{avg_diff_len_nltk:>20.3f}{avg_diff_len_gensim:>20.3f}\")\n",
    "    print(f\"{'Pct. documents with the most senteces:':>45}{frac_nltk_greater:>20.2%}{(1-frac_nltk_greater):>20.2%}\")\n",
    "    print('\\nExamples of differences:')\n",
    "    diff_list = []\n",
    "    printed = 0\n",
    "    for i,(orig,nltk_sent, gensim_sent) in diff_examples.iterrows():\n",
    "          if len(orig.split()) > 100:\n",
    "              continue\n",
    "          if printed >= 3:\n",
    "              break\n",
    "          if orig not in diff_list:\n",
    "              print('\\nORIGINAL:',orig)\n",
    "\n",
    "          print('NLTK:', nltk_sent)\n",
    "          print('GENSIM:', gensim_sent)\n",
    "          diff_list.append(orig)\n",
    "          printed += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_tokenize(sent):\n",
    "    return word_tokenize(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization with Spacy\n",
    "SpaCy provides models that are automatically pipelined, so one call will tokenize, tag, and identify named entities. For the purpose of comparison I will show how to only obtain the tokens. Spacy also has a clear API with easy instructions to alter the behavior of it's tokenization. I include functions here that include tokens unique to the Android corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenize(sent, model):\n",
    "    return [tok.text for tok in model(sent)]\n",
    "\n",
    "# Here I search for instances of a method or class (e.g. setStructuredData(String))\n",
    "def my_retokenizer_builder():\n",
    "    # methods\n",
    "    expressions = [r'[A-Za-z_]+(\\.[A-Za-z_]+)* ?\\( ?[A-Za-z_\\.]* ?\\)']\n",
    "    # Camel case w/ periods\n",
    "    expressions.append(r'(([A-Za-z_]*)\\.)+[A-Za-z_]*')\n",
    "    # General camel case \n",
    "    expressions.append(r'[A-Z]?([a-z]+[A-Z])+[a-z]+')\n",
    "    # Constants\n",
    "    expressions.append(r'[A-Z_]{4,}')\n",
    "\n",
    "    compiled_expressions = [re.compile(expression)  for expression in expressions]\n",
    "  \n",
    "    def my_retokenizer(doc):\n",
    "        for i,expression in enumerate(compiled_expressions):\n",
    "            doc = process_matches(doc, expression, ent_label='MT_OR_CL', tok_attrs={'POS' : 'PROPN'})\n",
    "\n",
    "        return doc\n",
    "    \n",
    "    return my_retokenizer\n",
    "\n",
    "\n",
    "def process_matches(doc, expression, ent_label=None, tok_attrs=None):\n",
    "    count=0\n",
    "    for match in re.finditer(expression, doc.text):\n",
    "        count+=1\n",
    "        if match.group(0) not in ['e.g.', 'i.e.']:\n",
    "            start,end = match.span()\n",
    "            span = doc.char_span(start, end, label = ent_label)\n",
    "            if span is not None:\n",
    "                pot_ents = [ent for ent in doc.ents if ent.start >= span.start or ent.end <= span.end]\n",
    "                if pot_ents != []:\n",
    "                    new_ents= list(doc.ents)\n",
    "                    [new_ents.remove(pe) for pe in pot_ents]\n",
    "                    doc.ents = new_ents\n",
    "                doc.ents = list(doc.ents) + [span]\n",
    "                with doc.retokenize() as retokenizer:\n",
    "                    retokenizer.merge(span, attrs=tok_attrs)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tagging w/ NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_pos(text):\n",
    "    return pos_tag(nltk_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tagging w/ SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_pos(sent, model):\n",
    "    return [tok.pos_ for tok in model(sent)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER w/ NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_ner(text):\n",
    "    return ne_chunk(nltk_pos(nltk_tokenize(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER w/ SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_ner(sent, model):\n",
    "    return [(ent.text,ent.label_) for ent in model(sent).ents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic similarity w/ NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic similarity w/ Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(re.split(r'[^\\(\\)]', 'hey ho( s)'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model_info():\n",
    "    if MODEL_COMPARISON_DIR in os.listdir():\n",
    "        filenames = [nltk_tok_f,spacy_tok_f,tok_diffs_f, nltk_pos_f, spacy_pos_f, nltk_ner_f, spacy_ner_f,parsed_docs_f]\n",
    "        variables = {'nltk_tok':None  ,'spacy_tok':None   ,'tok_diffs' :None  , 'nltk_pos':None   \n",
    "                     , 'spacy_pos' :None  , 'nltk_ner' :None  , 'spacy_ner':None ,'parsed_docs':None }\n",
    "\n",
    "        if len([f for f in filenames if f in os.listdir(MODEL_COMPARISON_DIR)]) == len(filenames):\n",
    "            for file, variable in zip(filenames, variables.keys()):\n",
    "                file_path = os.path.join(MODEL_COMPARISON_DIR,file)\n",
    "                variables[variable] = np.load(file_path, allow_pickle=True).tolist()\n",
    "#                 if variable == 'parsed_docs':\n",
    "#                     print('parsed_docs',variables[variable])\n",
    "#                 print(file_path)\n",
    "#                 print(variable)\n",
    "\n",
    "        tok_info = (variables['nltk_tok'], variables['spacy_tok'], variables['tok_diffs'])\n",
    "        pos_info = (variables['nltk_pos'],variables['spacy_pos'])\n",
    "        ner_info = (variables['nltk_ner'], variables['spacy_ner'])\n",
    "        return tok_info,pos_info, ner_info, variables['parsed_docs']\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def save_model_info(tok_info, pos_info, ner_info,parsed_docs):\n",
    "    nltk_tok, spacy_tok, tok_diffs = tok_info[0], tok_info[1], tok_info[2]\n",
    "    nltk_pos, spacy_pos = pos_info[0], pos_info[1]\n",
    "    nltk_ner, spacy_ner = ner_info[0], ner_info[1]\n",
    "    try:\n",
    "        os.makedirs(MODEL_COMPARISON_DIR)\n",
    "    except OSError as e:\n",
    "        if e.errno != errno.EEXIST:\n",
    "            raise\n",
    "    filenames = [nltk_tok_f,spacy_tok_f,tok_diffs_f, nltk_pos_f, spacy_pos_f, nltk_ner_f, spacy_ner_f, parsed_docs_f]\n",
    "    variables = [nltk_tok  ,spacy_tok  ,tok_diffs  , nltk_pos  , spacy_pos  , nltk_ner  , spacy_ner, parsed_docs]\n",
    "    for file, variable in zip(filenames, variables):\n",
    "        np_var = np.array(variable)\n",
    "        file_path = os.path.join(MODEL_COMPARISON_DIR,file)\n",
    "        print('Saving to:', file_path)\n",
    "        np.save(file_path, np_var)\n",
    "\n",
    "def model_comparison_info(df):\n",
    "    '''\n",
    "    This functions gets the token, pos tags, and named entities for all docs using both nltk and Spacy methods.\n",
    "    This is expensive to compute so the results are stored and later uploaded in liue of computation.\n",
    "    '''\n",
    "    model_info = load_model_info()\n",
    "    if model_info is not None:\n",
    "        return model_info\n",
    "    docs = df['docs']\n",
    "    methods = df['methods']\n",
    "    \n",
    "    nltk_tok, spacy_tok, nltk_pos, spacy_pos, nltk_ner, spacy_ner =[],[],[],[],[],[]\n",
    "    model = spacy.load('en_core_web_sm')\n",
    "    my_retokenizer = my_retokenizer_builder()\n",
    "    model.add_pipe(my_retokenizer, first=True)\n",
    "    total_docs = [(method,sent) for method,doc in zip(methods,sent_toks_gensim(docs)) for sent in doc]\n",
    "    total_docs_transpose = list(zip(*total_docs))\n",
    "    methods = total_docs_transpose[0]\n",
    "    docs = total_docs_transpose[1]\n",
    "    print('Documents to process:', len(docs))\n",
    "    parsed_docs = []\n",
    "    ##### THIS IS THE LOOP FOR PROCESSING DOCUMENTS #####\n",
    "    for i, doc in enumerate(model.pipe(docs, disable=[\"parser\"])):\n",
    "        if i %2000 ==0:\n",
    "            print('Processed',i)\n",
    "        nltk_sent_toks = nltk_tokenize(doc.text)\n",
    "        nltk_tok.append(nltk_sent_toks)\n",
    "        nltk_sent_pos_tags = pos_tag(nltk_sent_toks)\n",
    "        nltk_pos.append(nltk_sent_pos_tags)\n",
    "        nltk_sent_ne = [(' '.join([l[0] for l in ne.leaves()]),ne.label()) for ne in ne_chunk(nltk_sent_pos_tags)\n",
    "                        if type(ne) == nltk.tree.Tree]\n",
    "        nltk_ner.append(nltk_sent_ne)\n",
    "        \n",
    "        spacy_tok.append([tok.text for tok in doc])\n",
    "        spacy_pos.append([(tok.text,tok.pos_) for tok in doc])\n",
    "        spacy_ner.append([(ent.text, ent.label_) for ent in doc.ents])\n",
    "        \n",
    "        parsed_docs.append((methods[i],doc))\n",
    "        \n",
    "    tok_diffs = [(nltk_tok_sent,spacy_tok_sent) for nltk_tok_sent, spacy_tok_sent in zip(nltk_tok,spacy_tok)\n",
    "                                      if len(nltk_tok_sent) != len(spacy_tok_sent)]\n",
    "     \n",
    "    tok_info = (nltk_tok, spacy_tok, tok_diffs)\n",
    "    pos_info = (nltk_pos,spacy_pos)\n",
    "    ner_info = (nltk_ner, spacy_ner)\n",
    "    save_model_info(tok_info, pos_info, ner_info, parsed_docs)\n",
    "    return tok_info,pos_info, ner_info, parsed_docs\n",
    "\n",
    "def show_tok_diffs(nltk_tok, spacy_tok, diff_docs):\n",
    "    nltk_tot_tokens = sum([len(sent) for sent in nltk_tok])\n",
    "    spacy_tot_tokens = sum([len(sent) for sent in spacy_tok])\n",
    "    nltk_avg_tokens_tot = nltk_tot_tokens/len(nltk_tok)\n",
    "    spacy_avg_tokens_tot = spacy_tot_tokens/len(spacy_tok)\n",
    "    nltk_tot_toks_of_diff = sum([len(y) for y in [x for x in zip(*diff_docs)][0]])\n",
    "    spacy_tot_toks_of_diff = sum([len(y) for y in [x for x in zip(*diff_docs)][1]])\n",
    "    nltk_avg_tokens_diff = nltk_tot_toks_of_diff/len(diff_docs)\n",
    "    spacy_avg_tokens_diff = spacy_tot_toks_of_diff/len(diff_docs)\n",
    "    print(f'Number of documents with different tokenization: {len(diff_docs)} ', end='')\n",
    "    print(f'Fraction of total: {len(diff_docs)/ len(nltk_tok):.2%}')\n",
    "    print(f\"{'NLTK':>65}{'SpaCy':>20}\")\n",
    "    print(f\"{'Number of tokens:':>45}{nltk_tot_tokens:>20,}{spacy_tot_tokens:>20,}\")\n",
    "    print(f\"{'Avg. tokens per sentence:':>45}{nltk_avg_tokens_tot:>20,.3f}{spacy_avg_tokens_tot:>20,.3f}\")\n",
    "    print(f\"{'Number of tokens of differences:':>45}{nltk_tot_toks_of_diff:>20,}{spacy_tot_toks_of_diff:>20,}\")\n",
    "    print(f\"{'Avg. #tokens of differences:':>45}{nltk_avg_tokens_diff:>20.3f}{spacy_avg_tokens_diff:>20.3f}\")\n",
    "    print('\\nExamples of differences:')\n",
    "    pp = pprint.PrettyPrinter(indent=2, width=110)\n",
    "    printed = 0\n",
    "    for i, (nltk_toks, spacy_toks) in enumerate(diff_docs):\n",
    "          if len(nltk_toks) > 10 or len(spacy_toks) > 10:\n",
    "              continue\n",
    "          if printed >= 10:\n",
    "              break\n",
    "          print('\\nNLTK:')\n",
    "          pp.pprint(nltk_toks)\n",
    "          print('SpaCy:')\n",
    "          pp.pprint(spacy_toks)\n",
    "          printed+=1\n",
    "          \n",
    "          \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['android.app.assist.AssistContent.describeContents', Describe the kinds of special objects contained in this Parcelable instance's marshaled representation]\n"
     ]
    }
   ],
   "source": [
    "df, docs_by_method = get_docs(source_path)\n",
    "tok_info,pos_info, ner_info, parsed_docs = model_comparison_info(df.iloc[:100])\n",
    "nltk_toks, spacy_toks, tok_diffs = tok_info\n",
    "# print(len(tok_diffs))\n",
    "# show_tok_diffs(nltk_tok, spacy_tok, diff_docs)\n",
    "print(parsed_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Describe', 'the', 'kinds', 'of', 'special', 'objects', 'contained', 'in', 'this', 'Parcelable', 'instance', \"'s\", 'marshaled', 'representation']\n",
      "['Describe', 'the', 'kinds', 'of', 'special', 'objects', 'contained', 'in', 'this', 'Parcelable', 'instance', \"'s\", 'marshaled', 'representation']\n",
      "[('Describe', 'NNP'), ('the', 'DT'), ('kinds', 'NNS'), ('of', 'IN'), ('special', 'JJ'), ('objects', 'NNS'), ('contained', 'VBN'), ('in', 'IN'), ('this', 'DT'), ('Parcelable', 'JJ'), ('instance', 'NN'), (\"'s\", 'POS'), ('marshaled', 'JJ'), ('representation', 'NN')]\n",
      "[('Describe', 'VERB'), ('the', 'DET'), ('kinds', 'NOUN'), ('of', 'ADP'), ('special', 'ADJ'), ('objects', 'NOUN'), ('contained', 'VERB'), ('in', 'ADP'), ('this', 'DET'), ('Parcelable', 'ADJ'), ('instance', 'NOUN'), (\"'s\", 'PART'), ('marshaled', 'ADJ'), ('representation', 'NOUN')]\n",
      "[('Parcelable', 'ORGANIZATION')]\n",
      "[('Parcelable', 'ORG')]\n"
     ]
    }
   ],
   "source": [
    "print(nltk_toks[0])\n",
    "print(spacy_toks[0])\n",
    "print(pos_info[0][0])\n",
    "print(pos_info[1][0])\n",
    "print(ner_info[0][0])\n",
    "print(ner_info[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents with different tokenization: 50 Fraction of total: 28.90%\n",
      "                                                             NLTK               SpaCy\n",
      "                            Number of tokens:               2,649               2,599\n",
      "                    Avg. tokens per sentence:              15.312              15.023\n",
      "             Number of tokens of differences:                 884                 834\n",
      "                 Avg. #tokens of differences:              17.680              16.680\n",
      "\n",
      "Examples of differences:\n",
      "\n",
      "NLTK:\n",
      "['Can', 'be', 'modified', 'in-place']\n",
      "SpaCy:\n",
      "['Can', 'be', 'modified', 'in', '-', 'place']\n",
      "\n",
      "NLTK:\n",
      "['Returns', 'the', 'current', 'setStructuredData', '(', 'String', ')']\n",
      "SpaCy:\n",
      "['Returns', 'the', 'current', 'setStructuredData(String)']\n",
      "\n",
      "NLTK:\n",
      "['Return', 'a', 'Bundle', 'containing', 'optional', 'vendor-specific', 'extension', 'information']\n",
      "SpaCy:\n",
      "['Return', 'a', 'Bundle', 'containing', 'optional', 'vendor', '-', 'specific', 'extension', 'information']\n",
      "\n",
      "NLTK:\n",
      "['File', 'I/O', 'may', 'be', 'performed', 'in', 'this', 'call']\n",
      "SpaCy:\n",
      "['File', 'I', '/', 'O', 'may', 'be', 'performed', 'in', 'this', 'call']\n"
     ]
    }
   ],
   "source": [
    "# print(len(nltk_toks))\n",
    "# # print(nltk_toks)\n",
    "# tok_diffs = [(nltk_tok_sent,spacy_tok_sent) for nltk_tok_sent, spacy_tok_sent in zip(nltk_toks,spacy_toks)\n",
    "#                                       if len(nltk_tok_sent) != len(spacy_tok_sent)]\n",
    "show_tok_diffs(nltk_toks, spacy_toks, tok_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting sentencizer:\n",
      "Documents to process: 73154\n",
      "Processed 0\n",
      "Processed 2000\n",
      "Processed 4000\n",
      "Processed 6000\n",
      "Processed 8000\n",
      "Processed 10000\n",
      "Processed 12000\n",
      "Processed 14000\n",
      "Processed 16000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-d9296b49c3e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mspacy_ner\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Documents to process:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"parser\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ner\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tagger'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;36m2000\u001b[0m \u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Processed'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36mpipe\u001b[1;34m(self, texts, as_tuples, n_threads, batch_size, disable, cleanup, component_cfg, n_process)\u001b[0m\n\u001b[0;32m    814\u001b[0m         \u001b[0moriginal_strings_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    815\u001b[0m         \u001b[0mnr_seen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 816\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    817\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcleanup\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m_pipe\u001b[1;34m(docs, proc, kwargs)\u001b[0m\n\u001b[0;32m   1104\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1105\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1106\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1107\u001b[0m         \u001b[1;32myield\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-32-d1b910b1926d>\u001b[0m in \u001b[0;36mmy_retokenizer\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m#             print('expression:',i)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m#             print(expression)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_matches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpression\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ment_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'MT_OR_CL'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtok_attrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'POS'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;34m'PROPN'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df = get_docs(source_path)\n",
    "model = spacy.load('en_core_web_sm')\n",
    "my_retokenizer = my_retokenizer_builder()\n",
    "model.add_pipe(my_retokenizer, first=True)\n",
    "print('starting sentencizer:')\n",
    "docs = df['docs'].to_list()\n",
    "#docs = ['Hello my name is Chris. I like Computer Science.', 'I am an undergraduate android.Manifest.permission.MANAGE_ACCOUNTS.']\n",
    "docs = [sent for doc in sent_toks_gensim(docs) for sent in doc]\n",
    "# print(docs[5])\n",
    "# print(docs[500:521])\n",
    "nltk_toks = []\n",
    "spacy_toks = []\n",
    "nltk_pos = []\n",
    "spacy_pos = []\n",
    "nltk_ner = []\n",
    "spacy_ner =[]\n",
    "print('Documents to process:', len(docs))\n",
    "for i, doc in enumerate(model.pipe(docs, disable=[\"parser\", \"ner\", 'tagger'])):\n",
    "    if i %2000 ==0:\n",
    "        print('Processed',i)\n",
    "#     print('before',doc)\n",
    "    \n",
    "#     nltk_sent_toks = nltk_tokenize(doc.text)\n",
    "#     nltk_toks.append(nltk_sent_toks)\n",
    "#     nltk_sent_pos_tags = pos_tag(nltk_sent_toks)\n",
    "#     nltk_pos.append(nltk_sent_pos_tags)\n",
    "#     nltk_sent_ne = [(' '.join([l[0] for l in ne.leaves()]),ne.label()) for ne in ne_chunk(nltk_sent_pos_tags)\n",
    "#                     if type(ne) == nltk.tree.Tree]\n",
    "#     nltk_ner.append(nltk_sent_ne)\n",
    "\n",
    "    spacy_toks.append([tok.text for tok in doc])\n",
    "    spacy_pos.append([(tok.text,tok.pos_) for tok in doc])\n",
    "    spacy_ner.append([(ent.text, ent.label_) for ent in doc.ents])\n",
    "\n",
    "\n",
    "tok_diffs = [(nltk_tok_sent,spacy_tok_sent) for nltk_tok_sent, spacy_tok_sent in zip(nltk_toks,spacy_toks)\n",
    "                                  if len(nltk_tok_sent) != len(spacy_tok_sent)]\n",
    "\n",
    "show_tok_diffs(nltk_toks, spacy_toks, tok_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting sentencizer:\n",
      "\n",
      "[Requires, android, ., Manifest.permission, ., MANAGE_ACCOUNTS]\n",
      "z\n"
     ]
    }
   ],
   "source": [
    "df = get_docs(source_path)\n",
    "model = spacy.load('en_core_web_sm')\n",
    "#model.add_pipe(my_retokenizer, first=True)\n",
    "print('starting sentencizer:')\n",
    "docs = df['docs'].to_list()\n",
    "print(docs[555])\n",
    "my_retokenizer = my_retokenizer_builder()\n",
    "doc = my_retokenizer(model('Requires android.Manifest.permission.MANAGE_ACCOUNTS'))\n",
    "print([t for t in doc])\n",
    "# re.search(r'[A-Z_]{4,}',)\n",
    "print('z')\n",
    "# for match in re.finditer(r'([A-Z]+[a-z_]*)*\\( ?[A-Za-z_\\.]* ?\\)',docs[555]):\n",
    "#     print('blah')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fd744aac8dbd1e06bdf48a27c51b783a",
     "grade": false,
     "grade_id": "cell-01669f3b88a0d3c0",
     "locked": true,
     "points": 1.4,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b0a6b173e8037516e67ca3d0099d094d",
     "grade": false,
     "grade_id": "cell-e433e71229b731e8",
     "locked": true,
     "points": 1.4,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "## Presentation Graphic(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "481aeabfd3a0aaff418e1abcfd034be3",
     "grade": false,
     "grade_id": "cell-c075a70ffbe9d335",
     "locked": true,
     "points": 1.2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "## Project approach and overall execution\n",
    "Do not put anything below this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "643f1212dcfc2dbdc10f9965715144d3",
     "grade": false,
     "grade_id": "cell-307d558ea349391a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "## Code Structure and Organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9b36e1b21a5c9acc76ad001762eff01f",
     "grade": false,
     "grade_id": "cell-6a61ae6d12087891",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "## Code Commenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def doc_info(docs):\n",
    "    '''\n",
    "    Returns the parsed document, the token counter, POS tag counter,and the POS tag counter by word \n",
    "    '''\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", 'ner'])\n",
    "    nlp.add_pipe(my_retokenizer, first=True)\n",
    "    tok_cnt = Counter()\n",
    "    pos_cnt = Counter()\n",
    "    pos_byword_cnt = {}\n",
    "    parsed_docs = []\n",
    "    for tup in docs:\n",
    "        doc = nlp(tup[1])\n",
    "        parsed_docs.append((tup[0], doc))\n",
    "        for i, tok in enumerate(doc):\n",
    "            tok_cnt[tok.text] += 1\n",
    "            pos_cnt[tok.pos_] += 1\n",
    "            if tok.pos_ in pos_byword_cnt:\n",
    "                pos_byword_cnt[tok.pos_][tok.text] +=1\n",
    "            else:\n",
    "                pos_byword_cnt[tok.pos_] = Counter()\n",
    "                pos_byword_cnt[tok.pos_][tok.text] += 1\n",
    "                \n",
    "    return parsed_docs, tok_cnt, pos_cnt, pos_byword_cnt         \n",
    "\n",
    "\n",
    "def display_info(parsed_docs, method_documents,tok_cnt, pos_cnt, pos_byword_cnt, classes=None):\n",
    "    unique_sents = Counter()\n",
    "    unique_docs = []\n",
    "    for method,doc in parsed_docs:\n",
    "        if doc.text not in unique_sents:\n",
    "            unique_docs.append((method,doc))\n",
    "        unique_sents[doc.text] +=1  \n",
    "    if classes:\n",
    "        print('\\t>-total number of classes:', len(set(classes)), '<br>')\n",
    "        print('\\t -total number of methods:', len(method_documents.keys()), '<br>')\n",
    "    else:\n",
    "        print('\\t>-total number of methods:', len(method_documents.keys()), '<br>')\n",
    "    print('\\t -total records after transform:', len(parsed_docs), '<br>')\n",
    "    print('\\t -number of unique records after transform:', len(unique_sents), '<br>')\n",
    "    print('\\t -method with most sentences:', max([(key,len(method_documents[key])) for key in method_documents]\n",
    "                                             , key=lambda x: x[1]), '<br>')\n",
    "    print('\\t -method with most tokens:', max([(method,len(doc)) for method,doc in unique_docs]\n",
    "                                             , key=lambda x: x[1]), '<br>')\n",
    "    print('\\t -total number of tokens:', sum([tok_cnt[key] for key in tok_cnt.keys()]), '<br>')\n",
    "    print(\"\\t -num unique tokens:\", len(tok_cnt.keys()), '<br>')\n",
    "    print('\\t -most common tokens (with 5 or more chars):'\n",
    "          ,[tup for tup in tok_cnt.most_common() if len(tup[0])>4][:3], '<br>')\n",
    "    most_freq_pos = pos_cnt.most_common(1)[0][0]\n",
    "    print('\\t -most frequent POS tag:', most_freq_pos, '<br>')\n",
    "    print('\\t -most common words in that tag:', pos_byword_cnt[most_freq_pos].most_common(1)[0], '<br>')\n",
    "    print('\\t -most frequent proper noun:', pos_byword_cnt['PROPN'].most_common(1)[0], '<br>')\n",
    "    method_and_class_toks = [ent.text for p_doc in parsed_docs for ent in p_doc[1].ents \n",
    "                             if ent.label_ == 'MT_OR_CL']\n",
    "    print('\\t -number of unique domain-specific named entities:', len(method_and_class_toks), '<br>')\n",
    "    print('\\t -number of unique domain-specific named entities:', len(set(method_and_class_toks)), '<br>')\n",
    "    print('\\t -most frequent domain-specific named entity:'\n",
    "          , Counter(method_and_class_toks).most_common()[0], '<br>')\n",
    "    print()\n",
    "\n",
    "def process_doc_and_display_attrs(path, mappings_path=None):\n",
    "    docs, doc_by_method = access_data(path)  \n",
    "    classes = None\n",
    "    if mappings_path:\n",
    "        mapping_docs, _ = access_data(mappings_path) \n",
    "        mapped_methods = [mapping_doc[0] for mapping_doc in mapping_docs]\n",
    "        classes = [mapping_doc[1] for mapping_doc in mapping_docs]\n",
    "        docs = [doc for doc in docs if doc[0] in mapped_methods]\n",
    "        doc_by_method = {key:doc_by_method[key] for key in doc_by_method.keys() if key in mapped_methods}\n",
    "\n",
    "    parsed_docs, tok_cnt, pos_cnt, pos_byword_cnt = doc_info(docs)  \n",
    "    print('INFO FOR',path)\n",
    "    display_info(parsed_docs, doc_by_method, tok_cnt, pos_cnt, pos_byword_cnt, classes=classes)\n",
    "    return parsed_docs, tok_cnt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
