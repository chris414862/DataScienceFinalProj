{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f6de454b8e379477f0b24d4aeb46b54f",
     "grade": false,
     "grade_id": "cell-ea1356fb2fa161a8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Project Final Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c13f0fb473d6c832136a348847c3d245",
     "grade": true,
     "grade_id": "cell-520b9a1998578280",
     "locked": false,
     "points": 0.1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Objective statement:\n",
    "- I will create baseline classification standards to predict whether an Android API method manipulates/returns information associated with data types defined in privacy policies by using classical sparse vector space representations of the data with standard linear and non-linear classifiers.\n",
    "- As a secondary objective I will contrast using nltk for text processing and data exploration with using the newer spaCy and gensim packages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8afa10661e34a7e5d2ac29a291d29af8",
     "grade": false,
     "grade_id": "cell-259aa4719392cc9c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Obtaining the Data\n",
    "The data required for this project may be obtained as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6e9e40aaf5be6e8697f52969f9bf1edc",
     "grade": true,
     "grade_id": "cell-5d0765799a830601",
     "locked": false,
     "points": 0.1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "24df493960920d9ea471708f06fbace8",
     "grade": false,
     "grade_id": "cell-8770d1ab57c81792",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from collections import Counter\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.summarization.textcleaner import get_sentences\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "# from nltk.tokenize.api.StringTokenizer import tokenize\n",
    "\n",
    "datadir = 'proj_data'\n",
    "source_filename = 'android_semi_cleaned.csv'\n",
    "annotated_filename = 'android_cleaned_ANNOTATED_ONLY.csv'\n",
    "mappings_data_filename = 'mappings_cleaned.csv'\n",
    "source_path = os.path.join(datadir, source_filename)\n",
    "mappings_path = os.path.join(datadir, mappings_data_filename) \n",
    "annotated_path = os.path.join(datadir, annotated_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "619970a9d0e740b6cbcd0f1cbc67b906",
     "grade": false,
     "grade_id": "cell-5864d3b3e792fc82",
     "locked": true,
     "points": 1.1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def access_data(path):\n",
    "    with open(path,'r', newline='', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f, dialect='excel')\n",
    "        docs = []\n",
    "        methods = []\n",
    "        for i, row in enumerate(reader):\n",
    "            method = row[0]\n",
    "            doc = row[1] \n",
    "            methods.append(method)\n",
    "            docs.append(doc)\n",
    "        return methods, docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_html_remover(doc):\n",
    "    return re.sub('''</?\\w+((\\s+\\w+(\\s*=\\s*(?:\".*?\"|'.*?'|[\\^'\">\\s]+))?)+\\s*|\\s*)/?>''','', doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_cleaner(docs):\n",
    "    new_docs = []\n",
    "    for doc in docs:\n",
    "        # standardize spaces and newlines\n",
    "        new_doc = re.sub(r'\\s+', ' ', doc)\n",
    "        #new_doc = re.sub('''['\"]''', '', new_doc)\n",
    "        new_doc = my_html_remover(new_doc)\n",
    "        new_docs.append(new_doc)\n",
    "    return new_docs\n",
    "\n",
    "def get_docs(path):\n",
    "    methods, docs = access_data(path)\n",
    "    docs = my_cleaner(docs)\n",
    "    return pd.DataFrame({'docs': docs, 'methods':methods})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "Other cleaning such as punctuation removal and lemmatization will not be done here and will be considered later in the presentation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0a0e480232550900366ae2fc89b3dc39",
     "grade": false,
     "grade_id": "cell-141a2bb87d84a15e",
     "locked": true,
     "points": 1.2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of demonstration, I will show SpaCy's functionality in pieces, but normally text processing with SpaCy is pipelined automatically with options for minor alterations. The tag line from SpaCy is that _____ only the best tools for the job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence tokenization with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_toks_nltk(docs):\n",
    "    return [[s.strip(':. \"') for s in sent_tokenize(doc)] for doc in docs ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence tokenization with gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_toks_gensim(docs):\n",
    "    return [[s.strip(':. \"\\n') for s in get_sentences(doc)] for doc in docs ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence tokenization comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of documents: 29,402\n",
      "Number of differences: 527\n",
      "                                                             NLTK              Gensim\n",
      "                          Number of senteces:              73,158              73,154\n",
      "                 Avg. #tokens of differences:              18.139              20.767\n",
      "       Pct. documents with the most senteces:              51.10%              48.90%\n",
      "\n",
      "Examples of differences:\n",
      "\n",
      "ORIGINAL: Called by a device admin to set the short support message. This will be displayed to the user in settings screens where funtionality has been disabled by the admin. The message should be limited to a short statement such as \"This setting is disabled by your administrator. Contact someone@example.com for support.\" If the message is longer than 200 characters it may be truncated. If the short support message needs to be localized it is the responsibility of the DeviceAdminReceiver to listen to the Intent#ACTION_LOCALE_CHANGED broadcast and set a new version of this string accordingly.\n",
      "NLTK: Contact someone@example.com for support\n",
      "GENSIM: Contact someone@example.com for support.\" If the message is longer than 200 characters it may be truncated\n",
      "\n",
      "ORIGINAL: This method was deprecated in API level 21. use BluetoothLeScanner#startScan(List ScanSettings ScanCallback) instead. Starts a scan for Bluetooth LE devices looking for devices that advertise given services. Devices which advertise all specified services are reported using the LeScanCallback#onLeScan callback. Requires Manifest.permission.BLUETOOTH_ADMIN\n",
      "NLTK: This method was deprecated in API level 21. use BluetoothLeScanner#startScan(List ScanSettings ScanCallback) instead\n",
      "GENSIM: This method was deprecated in API level 21\n",
      "\n",
      "ORIGINAL: This method was deprecated in API level 21. use BluetoothLeScanner#startScan(List ScanSettings ScanCallback) instead. Starts a scan for Bluetooth LE devices. Results of the scan are reported using the LeScanCallback#onLeScan callback. Requires Manifest.permission.BLUETOOTH_ADMIN\n",
      "NLTK: This method was deprecated in API level 21. use BluetoothLeScanner#startScan(List ScanSettings ScanCallback) instead\n",
      "GENSIM: This method was deprecated in API level 21\n"
     ]
    }
   ],
   "source": [
    "def get_differences(orig_docs, nltk_docs, gensim_docs):\n",
    "    diff_cnt = 0\n",
    "    diff_orig, diff_nltk, diff_gensim = [],[],[]\n",
    "    same = []\n",
    "    for i, (orig_doc, nltk_doc, gensim_doc) in enumerate(zip(orig_docs,nltk_docs, gensim_docs)):        \n",
    "        # keep track of differences to prevent over-counting\n",
    "        diff_list=[]\n",
    "        \n",
    "        # must search sequentially to track where differences occur. Set operations would lose alignment\n",
    "        for i, (nltk_sent, gensim_sent) in enumerate(zip(nltk_doc, gensim_doc)):\n",
    "            \n",
    "            if (nltk_sent not in gensim_doc or gensim_sent not in nltk_doc) \\\n",
    "                and ([l for l in diff_list if gensim_sent in l or nltk_sent in l] == []):\n",
    "                diff_cnt += 1\n",
    "                diff_list.extend([nltk_sent,gensim_sent])\n",
    "                diff_orig.append(orig_doc)\n",
    "                diff_nltk.append(nltk_sent)\n",
    "                diff_gensim.append(gensim_sent)\n",
    "        \n",
    "            else:\n",
    "                same.append(gensim_sent)\n",
    "    diff_examples = pd.DataFrame({'Original':diff_orig,'nltk':diff_nltk,'gensim':diff_gensim})\n",
    "    \n",
    "    return diff_cnt, diff_examples, same\n",
    "        \n",
    "        \n",
    "def compare_sent_tokenization(docs):\n",
    "    \n",
    "    nltk_docs_tokenized = sent_toks_nltk(docs)\n",
    "    gensim_docs_tokenized = sent_toks_gensim(docs)\n",
    "    diff_cnt, diff_examples, _ = get_differences(docs, nltk_docs_tokenized, gensim_docs_tokenized)\n",
    "    num_nltk_sents = sum([len(d) for d in nltk_docs_tokenized])\n",
    "    num_gensim_sents = sum([len(d) for d in gensim_docs_tokenized])\n",
    "    avg_diff_len_nltk = sum([len(d.split()) for d in diff_examples['nltk']])/diff_cnt\n",
    "    avg_diff_len_gensim = sum([len(d.split()) for d in diff_examples['gensim']])/diff_cnt\n",
    "    diff_sent_lens = [(len(doc1),len(doc2)) for doc1,doc2 in zip(nltk_docs_tokenized,gensim_docs_tokenized)\\\n",
    "                                                if len(doc1) != len(doc2) ]\n",
    "    frac_nltk_greater = len(list(filter(lambda x:x[0]>x[1], diff_sent_lens)))/len(diff_sent_lens)\n",
    "    avg_diff_sents_nltk = sum([len(doc) for doc in nltk_docs_tokenized])/len(nltk_docs_tokenized)\n",
    "    avg_diff_sents_gensim = sum([len(doc) for doc in gensim_docs_tokenized])/len(gensim_docs_tokenized)\n",
    "#     avg_diff_sents_gensim = diff_examples.groupby('Original').count()\n",
    "#     print(avg_diff_sents_nltk)\n",
    "    print(f\"Total number of documents: {len(docs):,}\")\n",
    "    print(f\"Number of differences: {diff_cnt:,}\")\n",
    "    print(f\"{'NLTK':>65}{'Gensim':>20}\")\n",
    "    print(f\"{'Number of senteces:':>45}{num_nltk_sents:>20,}{num_gensim_sents:>20,}\")\n",
    "    print(f\"{'Avg. #tokens of differences:':>45}{avg_diff_len_nltk:>20.3f}{avg_diff_len_gensim:>20.3f}\")\n",
    "    print(f\"{'Pct. documents with the most senteces:':>45}{frac_nltk_greater:>20.2%}{(1-frac_nltk_greater):>20.2%}\")\n",
    "    print('\\nExamples of differences:')\n",
    "    diff_list = []\n",
    "    printed = 0\n",
    "    for i,(orig,nltk_sent, gensim_sent) in diff_examples.iterrows():\n",
    "          if len(orig.split()) > 100:\n",
    "              continue\n",
    "          if printed >= 3:\n",
    "              break\n",
    "          if orig not in diff_list:\n",
    "              print('\\nORIGINAL:',orig)\n",
    "\n",
    "          print('NLTK:', nltk_sent)\n",
    "          print('GENSIM:', gensim_sent)\n",
    "          diff_list.append(orig)\n",
    "          printed += 1\n",
    "    \n",
    "df = get_docs(source_path)\n",
    "compare_sent_tokenization(df['docs'].to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_tokenize(sent):\n",
    "    return word_tokenize(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization with Spacy\n",
    "SpaCy provides models that are automatically pipelined, so one call will tokenize, tag, and identify named entities. For the purpose of comparison I will separate them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenize(sent, model):\n",
    "    return [tok.text for tok in model(sent)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "def compare_tokenization(docs):\n",
    "    nltk_tok_by_sent = []\n",
    "    spacy_tok_by_sent = []\n",
    "    model = spacy.load('en_core_web_sm')\n",
    "    for i, doc in enumerate(model.pipe(docs, disable=[\"tagger\", \"parser\", \"ner\"])):\n",
    "        if i %5000 ==0:\n",
    "            print(i)\n",
    "        nltk_tok_by_sent.append(nltk_tokenize(doc.text))\n",
    "    #print(model.pipe_names)\n",
    "    #for doc in model.pipe(docs, disable=[\"tagger\", \"parser\", \"ner\"]):\n",
    "        spacy_tok_by_sent.append([tok.text for tok in doc])\n",
    "        \n",
    "    diff_docs = [(nltk_toks,spacy_toks) for nltk_toks, spacy_toks in zip(nltk_tok_by_sent,spacy_tok_by_sent)\n",
    "                                      if len(nltk_toks) != len(spacy_toks)]\n",
    "    return nltk_tok_by_sent, spacy_tok_by_sent, diff_docs\n",
    "df = get_docs(source_path)\n",
    "nltk_tok_by_sent, spacy_tok_by_sent, diff_docs = compare_tokenization(df['docs'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents with different tokenization: 9839 Fraction of total: 33.46%\n",
      "                                                             NLTK               SpaCy\n",
      "                            Number of tokens:           1,262,017           1,262,671\n",
      "                    Avg. tokens per sentence:              42.923              42.945\n",
      "             Number of tokens of differences:             769,146             769,800\n",
      "                 Avg. #tokens of differences:              78.173              78.240\n",
      "\n",
      "Examples of differences:\n",
      "\n",
      "NLTK:\n",
      "['Returns', 'the', 'current', 'setStructuredData', '(', 'String', ')', '.']\n",
      "SpaCy:\n",
      "['Returns', 'the', 'current', 'setStructuredData(String', ')', '.']\n",
      "\n",
      "NLTK:\n",
      "['Returns', 'the', 'system-wide', 'Private', 'DNS', 'host', '.']\n",
      "SpaCy:\n",
      "['Returns', 'the', 'system', '-', 'wide', 'Private', 'DNS', 'host', '.']\n",
      "\n",
      "NLTK:\n",
      "['Returns', 'the', 'system-wide', 'Private', 'DNS', 'mode', '.']\n",
      "SpaCy:\n",
      "['Returns', 'the', 'system', '-', 'wide', 'Private', 'DNS', 'mode', '.']\n",
      "\n",
      "NLTK:\n",
      "['Callback', 'for', 'AccessibilityEvent', 's', '.']\n",
      "SpaCy:\n",
      "['Callback', 'for', 'AccessibilityEvent', 's.']\n",
      "\n",
      "NLTK:\n",
      "['Get', 'the', 'recommended', 'timeout', 'for', 'non-interactive', 'controls', '.']\n",
      "SpaCy:\n",
      "['Get', 'the', 'recommended', 'timeout', 'for', 'non', '-', 'interactive', 'controls', '.']\n",
      "\n",
      "NLTK:\n",
      "['Convenience', 'for', 'calling', 'getParcelFileDescriptor', '(', ')', '.close', '(', ')', '.']\n",
      "SpaCy:\n",
      "['Convenience', 'for', 'calling', 'getParcelFileDescriptor().close', '(', ')', '.']\n",
      "\n",
      "NLTK:\n",
      "[ 'Confirm',\n",
      "  'passkey',\n",
      "  'for',\n",
      "  'PAIRING_VARIANT_PASSKEY_CONFIRMATION',\n",
      "  'pairing',\n",
      "  '.',\n",
      "  'Requires',\n",
      "  'Manifest.permission.BLUETOOTH_PRIVILEGED']\n",
      "SpaCy:\n",
      "[ 'Confirm',\n",
      "  'passkey',\n",
      "  'for',\n",
      "  'PAIRING_VARIANT_PASSKEY_CONFIRMATION',\n",
      "  'pairing',\n",
      "  '.',\n",
      "  'Requires',\n",
      "  'Manifest.permission',\n",
      "  '.',\n",
      "  'BLUETOOTH_PRIVILEGED']\n",
      "\n",
      "NLTK:\n",
      "['Callback', 'triggered', 'as', 'result', 'of', 'BluetoothGatt', '#', 'readPhy']\n",
      "SpaCy:\n",
      "['Callback', 'triggered', 'as', 'result', 'of', 'BluetoothGatt#readPhy']\n",
      "\n",
      "NLTK:\n",
      "['Callback', 'triggered', 'as', 'result', 'of', 'BluetoothGattServer', '#', 'readPhy']\n",
      "SpaCy:\n",
      "['Callback', 'triggered', 'as', 'result', 'of', 'BluetoothGattServer#readPhy']\n",
      "\n",
      "NLTK:\n",
      "['SQL-escape', 'a', 'string', '.']\n",
      "SpaCy:\n",
      "['SQL', '-', 'escape', 'a', 'string', '.']\n"
     ]
    }
   ],
   "source": [
    "def show_diffs(nltk_tok_by_sent, spacy_tok_by_sent, diff_docs):\n",
    "    nltk_tot_tokens = sum([len(sent) for sent in nltk_tok_by_sent])\n",
    "    spacy_tot_tokens = sum([len(sent) for sent in spacy_tok_by_sent])\n",
    "    nltk_avg_tokens_tot = nltk_tot_tokens/len(nltk_tok_by_sent)\n",
    "    spacy_avg_tokens_tot = spacy_tot_tokens/len(spacy_tok_by_sent)\n",
    "#     print([len(sent) for sent in diff_docs)\n",
    "    nltk_tot_toks_of_diff = sum([len(y) for y in [x for x in zip(*diff_docs)][0]])\n",
    "    spacy_tot_toks_of_diff = sum([len(y) for y in [x for x in zip(*diff_docs)][1]])\n",
    "    nltk_avg_tokens_diff = nltk_tot_toks_of_diff/len(diff_docs)\n",
    "    spacy_avg_tokens_diff = spacy_tot_toks_of_diff/len(diff_docs)\n",
    "    print(f'Number of documents with different tokenization: {len(diff_docs)} ', end='')\n",
    "    print(f'Fraction of total: {len(diff_docs)/ len(nltk_tok_by_sent):.2%}')\n",
    "    print(f\"{'NLTK':>65}{'SpaCy':>20}\")\n",
    "    print(f\"{'Number of tokens:':>45}{nltk_tot_tokens:>20,}{spacy_tot_tokens:>20,}\")\n",
    "    print(f\"{'Avg. tokens per sentence:':>45}{nltk_avg_tokens_tot:>20,.3f}{spacy_avg_tokens_tot:>20,.3f}\")\n",
    "    print(f\"{'Number of tokens of differences:':>45}{nltk_tot_toks_of_diff:>20,}{spacy_tot_toks_of_diff:>20,}\")\n",
    "    print(f\"{'Avg. #tokens of differences:':>45}{nltk_avg_tokens_diff:>20.3f}{spacy_avg_tokens_diff:>20.3f}\")\n",
    "    print('\\nExamples of differences:')\n",
    "    pp = pprint.PrettyPrinter(indent=2, width=110)\n",
    "    printed = 0\n",
    "    for i, (nltk_toks, spacy_toks) in enumerate(diff_docs):\n",
    "          if len(nltk_toks) > 10 or len(spacy_toks) > 10:\n",
    "              continue\n",
    "          if printed >= 10:\n",
    "              break\n",
    "          print('\\nNLTK:')\n",
    "          pp.pprint(nltk_toks)\n",
    "          print('SpaCy:')\n",
    "          pp.pprint(spacy_toks)\n",
    "          printed+=1\n",
    "show_diffs(nltk_tok_by_sent, spacy_tok_by_sent, diff_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tagging w/ NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tagging w/ SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tagging comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER w/ NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER w/ SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic similarity w/ NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic similarity w/ SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic similarity comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_matches(doc, expression, ent_label=None, tok_attrs=None):\n",
    "    for match in re.finditer(expression, doc.text):\n",
    "        if match.group(0) not in ['e.g.', 'i.e.']:\n",
    "            start,end = match.span()\n",
    "            span = doc.char_span(start, end, label = ent_label)\n",
    "            if span is not None:\n",
    "                doc.ents = list(doc.ents) + [span]\n",
    "                with doc.retokenize() as retokenizer:\n",
    "                    retokenizer.merge(span, attrs=tok_attrs)\n",
    "    return doc\n",
    "\n",
    "\n",
    "def my_retokenizer(doc):\n",
    "    expression = r'([A-Za-z]+\\.)+[A-Za-z]+\\.?'\n",
    "    doc = process_matches(doc, expression, ent_label='MT_OR_CL', tok_attrs={'POS' : 'PROPN'})\n",
    "    expression = r'[A-Z]*[a-z]+([A-Z]+[a-z]*)+'\n",
    "    doc = process_matches(doc, expression, ent_label='MT_OR_CL', tok_attrs={'POS' : 'PROPN'})\n",
    "    return doc\n",
    "\n",
    "\n",
    "def doc_info(docs):\n",
    "    '''\n",
    "    Returns the parsed document, the token counter, POS tag counter,and the POS tag counter by word \n",
    "    '''\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", 'ner'])\n",
    "    nlp.add_pipe(my_retokenizer, first=True)\n",
    "    tok_cnt = Counter()\n",
    "    pos_cnt = Counter()\n",
    "    pos_byword_cnt = {}\n",
    "    parsed_docs = []\n",
    "    for tup in docs:\n",
    "        doc = nlp(tup[1])\n",
    "        parsed_docs.append((tup[0], doc))\n",
    "        for i, tok in enumerate(doc):\n",
    "            tok_cnt[tok.text] += 1\n",
    "            pos_cnt[tok.pos_] += 1\n",
    "            if tok.pos_ in pos_byword_cnt:\n",
    "                pos_byword_cnt[tok.pos_][tok.text] +=1\n",
    "            else:\n",
    "                pos_byword_cnt[tok.pos_] = Counter()\n",
    "                pos_byword_cnt[tok.pos_][tok.text] += 1\n",
    "                \n",
    "    return parsed_docs, tok_cnt, pos_cnt, pos_byword_cnt         \n",
    "\n",
    "\n",
    "def display_info(parsed_docs, method_documents,tok_cnt, pos_cnt, pos_byword_cnt, classes=None):\n",
    "    unique_sents = Counter()\n",
    "    unique_docs = []\n",
    "    for method,doc in parsed_docs:\n",
    "        if doc.text not in unique_sents:\n",
    "            unique_docs.append((method,doc))\n",
    "        unique_sents[doc.text] +=1  \n",
    "    if classes:\n",
    "        print('\\t>-total number of classes:', len(set(classes)), '<br>')\n",
    "        print('\\t -total number of methods:', len(method_documents.keys()), '<br>')\n",
    "    else:\n",
    "        print('\\t>-total number of methods:', len(method_documents.keys()), '<br>')\n",
    "    print('\\t -total records after transform:', len(parsed_docs), '<br>')\n",
    "    print('\\t -number of unique records after transform:', len(unique_sents), '<br>')\n",
    "    print('\\t -method with most sentences:', max([(key,len(method_documents[key])) for key in method_documents]\n",
    "                                             , key=lambda x: x[1]), '<br>')\n",
    "    print('\\t -method with most tokens:', max([(method,len(doc)) for method,doc in unique_docs]\n",
    "                                             , key=lambda x: x[1]), '<br>')\n",
    "    print('\\t -total number of tokens:', sum([tok_cnt[key] for key in tok_cnt.keys()]), '<br>')\n",
    "    print(\"\\t -num unique tokens:\", len(tok_cnt.keys()), '<br>')\n",
    "    print('\\t -most common tokens (with 5 or more chars):'\n",
    "          ,[tup for tup in tok_cnt.most_common() if len(tup[0])>4][:3], '<br>')\n",
    "    most_freq_pos = pos_cnt.most_common(1)[0][0]\n",
    "    print('\\t -most frequent POS tag:', most_freq_pos, '<br>')\n",
    "    print('\\t -most common words in that tag:', pos_byword_cnt[most_freq_pos].most_common(1)[0], '<br>')\n",
    "    print('\\t -most frequent proper noun:', pos_byword_cnt['PROPN'].most_common(1)[0], '<br>')\n",
    "    method_and_class_toks = [ent.text for p_doc in parsed_docs for ent in p_doc[1].ents \n",
    "                             if ent.label_ == 'MT_OR_CL']\n",
    "    print('\\t -number of unique domain-specific named entities:', len(method_and_class_toks), '<br>')\n",
    "    print('\\t -number of unique domain-specific named entities:', len(set(method_and_class_toks)), '<br>')\n",
    "    print('\\t -most frequent domain-specific named entity:'\n",
    "          , Counter(method_and_class_toks).most_common()[0], '<br>')\n",
    "    print()\n",
    "\n",
    "def process_doc_and_display_attrs(path, mappings_path=None):\n",
    "    docs, doc_by_method = access_data(path)  \n",
    "    classes = None\n",
    "    if mappings_path:\n",
    "        mapping_docs, _ = access_data(mappings_path) \n",
    "        mapped_methods = [mapping_doc[0] for mapping_doc in mapping_docs]\n",
    "        classes = [mapping_doc[1] for mapping_doc in mapping_docs]\n",
    "        docs = [doc for doc in docs if doc[0] in mapped_methods]\n",
    "        doc_by_method = {key:doc_by_method[key] for key in doc_by_method.keys() if key in mapped_methods}\n",
    "\n",
    "    parsed_docs, tok_cnt, pos_cnt, pos_byword_cnt = doc_info(docs)  \n",
    "    print('INFO FOR',path)\n",
    "    display_info(parsed_docs, doc_by_method, tok_cnt, pos_cnt, pos_byword_cnt, classes=classes)\n",
    "    return parsed_docs, tok_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fd744aac8dbd1e06bdf48a27c51b783a",
     "grade": false,
     "grade_id": "cell-01669f3b88a0d3c0",
     "locked": true,
     "points": 1.4,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b0a6b173e8037516e67ca3d0099d094d",
     "grade": false,
     "grade_id": "cell-e433e71229b731e8",
     "locked": true,
     "points": 1.4,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "## Presentation Graphic(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "481aeabfd3a0aaff418e1abcfd034be3",
     "grade": false,
     "grade_id": "cell-c075a70ffbe9d335",
     "locked": true,
     "points": 1.2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "## Project approach and overall execution\n",
    "Do not put anything below this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "643f1212dcfc2dbdc10f9965715144d3",
     "grade": false,
     "grade_id": "cell-307d558ea349391a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "## Code Structure and Organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9b36e1b21a5c9acc76ad001762eff01f",
     "grade": false,
     "grade_id": "cell-6a61ae6d12087891",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "## Code Commenting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
