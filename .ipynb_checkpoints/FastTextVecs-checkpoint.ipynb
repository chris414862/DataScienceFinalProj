{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import fasttext\n",
    "import os\n",
    "import re\n",
    "import gensim\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from gensim.summarization.textcleaner import get_sentences\n",
    "from gensim.summarization.textcleaner import clean_text_by_sentences\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lookups import Lookups\n",
    "import random\n",
    "\n",
    "source_filename = 'android_semi_cleaned.csv'\n",
    "annotated_filename = 'android_cleaned_ANNOTATED_ONLY.csv'\n",
    "data_dir = os.path.join(\"proj_data\",\"cleaned_data\")\n",
    "mappings_data_filename = 'mappings_cleaned.csv'\n",
    "source_path = os.path.join(data_dir, source_filename)\n",
    "mappings_path = os.path.join(data_dir, mappings_data_filename) \n",
    "annotated_path = os.path.join(data_dir, annotated_filename)\n",
    "prepped_file = 'prepped.csv'\n",
    "save_name = 'ft.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(source_file, 'r', newline='', encoding='utf-8') as f:\n",
    "#     reader = csv.reader(f, dialect='excel')\n",
    "#     docs1 = []\n",
    "#     docs2 = []\n",
    "#     methods = []\n",
    "#     diff = 0\n",
    "#     for i, row in enumerate(reader):\n",
    "\n",
    "#         gen = get_sentences(re.sub(r'\\s+', ' ', row[1]))\n",
    "#         doc1 = [d for d in gen]\n",
    "#         doc2 = sent_tokenize(re.sub(r'\\s+', ' ', row[1]))\n",
    "#         flag = True\n",
    "#         for i, (sent1, sent2) in enumerate(zip(doc1, doc2)):\n",
    "#             diff_list=[]\n",
    "#             if sent1 not in doc2 or  sent2 not in doc1 and sent1 not in diff_list and sent2 not in diff_list:\n",
    "#                 if flag:\n",
    "#                     print('ORIGINAL: ', re.sub(r'\\s+', ' ', row[1]))\n",
    "#                     flag = False\n",
    "#                 print('GENSIM: ', sent1)\n",
    "#                 print('NLTK: ', sent2)\n",
    "#                 diff+= 1\n",
    "#                 diff_list.extend([sent1,sent2])\n",
    "#         if not flag:\n",
    "#             print('\\n')\n",
    "#         docs1.extend(doc1)#doc.lower().rstrip('.'))\n",
    "#         docs2.extend(doc2)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def access_data(path):\n",
    "    with open(path,'r', newline='', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f, dialect='excel')\n",
    "        docs = []\n",
    "        methods = []\n",
    "        full = []\n",
    "        for i, row in enumerate(reader):\n",
    "            method = row[0]\n",
    "            doc = row[1] \n",
    "            full.append(tuple(row))\n",
    "            methods.append(method)\n",
    "            docs.append(doc)\n",
    "        return methods, docs, full\n",
    "\n",
    "def process_matches(doc, expression, ent_label=None, tok_attrs=None):\n",
    "    for match in re.finditer(expression, doc.text):\n",
    "        if match.group(0) not in ['e.g.', 'i.e.']:\n",
    "            start,end = match.span()\n",
    "            span = doc.char_span(start, end, label = ent_label)\n",
    "            if span is not None:\n",
    "                pot_ents = [ent for ent in doc.ents if ent.start >= span.start or ent.end <= span.end]\n",
    "                if pot_ents != []:\n",
    "                    new_ents= list(doc.ents)\n",
    "                    [new_ents.remove(pe) for pe in pot_ents]\n",
    "                    doc.ents = new_ents\n",
    "                doc.ents = list(doc.ents) + [span]\n",
    "                with doc.retokenize() as retokenizer:\n",
    "                    retokenizer.merge(span, attrs=tok_attrs)\n",
    "    return doc\n",
    "\n",
    "\n",
    "def my_retokenizer(doc):\n",
    "    expression = r'([A-Za-z]+\\.)+[A-Za-z]+\\.?(\\( ?[A-Za-z\\.]* ?\\))'\n",
    "    doc = process_matches(doc, expression, ent_label='MT_OR_CL', tok_attrs={'POS' : 'PROPN'})\n",
    "    expression = r'[A-Z]*[a-z]+([A-Z]+[a-z]*)*(\\( ?[A-Za-z\\.]* ?\\))'\n",
    "    doc = process_matches(doc, expression, ent_label='MT_OR_CL', tok_attrs={'POS' : 'PROPN'})\n",
    "    return doc\n",
    "\n",
    "def process_file(path, mappings_path=None):\n",
    "    '''\n",
    "    Returns a list of two-tuples. The first entry is a label, the second is a document\n",
    "    '''\n",
    "    method_labels, docs, full_rows = access_data(path)  \n",
    "    classes = None\n",
    "    if mappings_path:\n",
    "        mapped_methods, lex_class, _ = access_data(mappings_path) \n",
    "        #mapped_methods = [mapping_doc[0] for mapping_doc in mapping_docs]\n",
    "        #classes = [mapping_doc[1] for mapping_doc in mapping_docs]\n",
    "        #full_rows = [row ifor row in full_rows ]\n",
    "        method_labels = [row[0] if row[0] in mapped_methods else 'NoClass' for row in full_rows]\n",
    "        docs = [row[1] for row in full_rows]\n",
    "        #doc_by_method = {key:doc_by_method[key] for key in doc_by_method.keys() if key in mapped_methods}\n",
    "        \n",
    "    return method_labels, docs, full_rows\n",
    "\n",
    "def my_sentencizer(docs, labels = None):\n",
    "    \n",
    "    if labels and len(labels) != len(docs):\n",
    "        raise ValueError('Docs and labels are not the same length')\n",
    "        \n",
    "    new_docs = []\n",
    "    new_labels = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        sents = [sent for sent in get_sentences(doc) if sent != '']\n",
    "        new_docs.extend(sents)\n",
    "        if labels:\n",
    "            new_labels.extend([labels[i]]*len(sents))\n",
    "            \n",
    "#         if i < 20:\n",
    "#             print('\\norig:')\n",
    "#             print(doc)\n",
    "#             print('new:')\n",
    "#             print(sents)\n",
    "#             print('orig label')\n",
    "#             print(labels[i])\n",
    "#             print([labels[i]]*len(sents))\n",
    "#             print('new labels:')\n",
    "#             print(new_labels)\n",
    "#         if i %2000 ==0:\n",
    "#             print('\\nsentencizer check:',i)\n",
    "#             print(new_docs)\n",
    "# #             print(new_labels)\n",
    "#     print('done sentencizing')   \n",
    "    if labels:\n",
    "        new_docs = [(a,b) for a,b in zip(new_docs, new_labels)]\n",
    "        \n",
    "    return new_docs\n",
    "\n",
    "def my_cleaner(docs):\n",
    "    '''\n",
    "    This method separates a list of documents into a list of sentences with equal spaces. If maintain label\n",
    "    is True, the first column (the label column) will be dublicated for each new sentence. The result will \n",
    "    be a list of tuples, with the first entry as the label and the second as the sentence\n",
    "    '''\n",
    "    new_docs = []\n",
    "    for doc in docs:\n",
    "        # standardize spaces and newlines\n",
    "        new_doc = re.sub(r'\\s+', ' ', doc)\n",
    "        new_docs.append(new_doc)\n",
    "\n",
    "    return new_docs\n",
    "\n",
    "def load_classification_docs(doc_path, mappings_path):\n",
    "    meth_labels,docs, full_rows = process_file(doc_path, mappings_path)\n",
    "    docs = my_cleaner(docs)\n",
    "    learning_examples = my_sentencizer(docs, meth_labels)\n",
    "    random.shuffle(learning_examples)\n",
    "    training = learning_examples[:int(len(learning_examples)*.7)]\n",
    "    testing = learning_examples[int(len(learning_examples)*.7):]\n",
    "    trainX = [a for a, b in training]\n",
    "    trainY = [b for a, b in training]\n",
    "    testX = [a for a, b in testing]\n",
    "    testY = [b for a, b in testing]\n",
    "    \n",
    "    return trainX, trainY, testX, testY\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = spacy.load('en_core_web_lg')#'en_core_web_sm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainY, testX, testY = load_classification_docs(annotated_path, mappings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    model.add_pipe(my_retokenizer, first=True)\n",
    "except ValueError:\n",
    "    model.remove_pipe('my_retokenizer')\n",
    "    model.add_pipe(my_retokenizer, first=True)\n",
    "    \n",
    "processed_docs = []\n",
    "for doc in model.pipe(trainX, disable=['parser', 'tagger', 'ner']):\n",
    "    processed_docs.append(doc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "oovs = [tok.text for doc in processed_docs for tok in doc if tok.is_oov and not tok.is_punct]\n",
    "oovs_lem = [tok.lemma_ for doc in processed_docs for tok in doc if tok.is_oov and not tok.is_punct]\n",
    "lemmas = [tok.lemma_ for doc in processed_docs for tok in doc if not tok.is_punct]\n",
    "toks = [tok.text for doc in processed_docs for tok in doc]\n",
    "toks_no_punct = [tok.text for doc in processed_docs for tok in doc if not tok.is_punct]\n",
    "oov_count = Counter(oovs)\n",
    "oov_lem_count = Counter(oovs_lem)\n",
    "tok_count = Counter(toks)\n",
    "tok_no_punct_count = Counter(toks_no_punct)\n",
    "lemma_count = Counter(lemmas)\n",
    "lemma_diff_toks_oov = [lemma for lemma in oov_lem_count.keys() if lemma not in list(oov_count.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens 216558\n",
      "Total tokens (not punctuation) 193132\n",
      "Total unique tokens 7954\n",
      "Total unique tokens (no punctuation) 7926\n",
      "Total unique lemmas 6603\n",
      "Out of vocab tokens: 8397\n",
      "Out of vocab unique tokens: 2542\n",
      "Out of vocab unique lemmas: 2542\n",
      "Most common out of vocab tokens: [('number', 156), ('Manifest.permission', 140), ('Drawable', 90), ('ID', 89), ('PendingIntent', 85), ('com.android.contacts', 68), ('android.graphics', 43), ('BroadcastReceiver', 40), ('meta', 40), ('android.content', 39), ('dataPosition()', 36), ('ShowAll', 34), ('LayoutParams', 31), ('X', 30), ('root', 29), ('prepare()', 28), ('VERSION_CODES.P', 27), ('\\\\([0', 27), ('WindowManager', 26), ('mutate()', 26)]\n",
      "Most common out of vocab lemmas: [('numb', 156), ('Manifest.permission', 140), ('Drawable', 90), ('ID', 89), ('PendingIntent', 85), ('com.android.contacts', 68), ('android.graphics', 43), ('BroadcastReceiver', 40), ('meta', 40), ('android.content', 39), ('dataPosition()', 36), ('ShowAll', 34), ('LayoutParams', 31), ('X', 30), ('root', 29), ('prepare()', 28), ('VERSION_CODES.P', 27), ('\\\\([0', 27), ('WindowManager', 26), ('mutate()', 26)]\n",
      "OOV lemmas not in tokens: ['numb']\n"
     ]
    }
   ],
   "source": [
    "print('Total tokens', len(toks))\n",
    "print('Total tokens (not punctuation)', len(toks_no_punct))\n",
    "print('Total unique tokens', len(tok_count.keys()))\n",
    "print('Total unique tokens (no punctuation)', len(tok_no_punct_count))\n",
    "print('Total unique lemmas', len(lemma_count))\n",
    "print('Out of vocab tokens:', len(oovs))\n",
    "print('Out of vocab unique tokens:', len(oov_count))\n",
    "print('Out of vocab unique lemmas:', len(oov_lem_count))\n",
    "\n",
    "print('Most common out of vocab tokens:', oov_count.most_common(20))\n",
    "print('Most common out of vocab lemmas:', oov_lem_count.most_common(20))\n",
    "print('OOV lemmas not in tokens:', lemma_diff_toks_oov[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "called\n",
      "equals(Object )\n",
      "and\n",
      "received\n",
      "x\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    textcat = nlp.create_pipe(\n",
    "            \"textcat\", config={\"exclusive_classes\": False, \"architecture\": \"simple_cnn\"}\n",
    "        )\n",
    "except:\n",
    "    model.remove_pipe('textcat')\n",
    "        textcat = nlp.create_pipe(\n",
    "            \"textcat\", config={\"exclusive_classes\": False, \"architecture\": \"simple_cnn\"}\n",
    "        )\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Describe the kinds of special objects contained in this Parcelable instance 's marshaled representation .\n",
      "For example if the object will include a file descriptor in the output of writeToParcel(android.os . Parcel int ) the return value of this method must include the CONTENTS_FILE_DESCRIPTOR bit .\n",
      "Return the current setClipData(ClipData) which you can modify in - place .\n",
      "Return Bundle for extra vendor - specific data that can be modified and examined .\n",
      "Returns the current setIntent(Intent) if one is set else the default Intent obtained from Activity.getIntent .\n",
      "Can be modified in - place .\n",
      "Returns the current setStructuredData(String) .\n",
      "Return the content 's web URI as per setWebUri(android.net.Uri) or null if there is none .\n",
      "Returns whether or not the current Intent was explicitly provided in Activity.onProvideAssistContent .\n",
      "If not the Intent was automatically set based on Activity.getIntent .\n",
      "Returns whether or not the current getWebUri() was explicitly provided in Activity.onProvideAssistContent .\n",
      "If not the Intent was automatically set based on Activity.getIntent .\n",
      "Optional additional content items that are involved with the current UI .\n",
      "Access to this content will be granted to the assistant as if you are sending it through an Intent with Intent#FLAG_GRANT_READ_URI_PERMISSION .\n",
      "Sets the Intent associated with the content describing the current top - level context of the activity .\n",
      "If this contains a reference to a piece of data related to the activity be sure to set Intent#FLAG_GRANT_READ_URI_PERMISSION so the accessibility service can access it .\n",
      "Sets optional structured data regarding the content being viewed .\n",
      "The provided data must be a string represented with JSON - LD using the schema.org vocabulary .\n",
      "Set a web URI associated with the current data being shown to the user .\n",
      "This URI could be opened in a web browser or in the app as an Intent#ACTION_VIEW Intent to show the same data that is currently being displayed by it .\n"
     ]
    }
   ],
   "source": [
    "prepped_texts = [' '.join([tok.text for tok in doc]) for doc in processed_docs]\n",
    "with open(prepped_file, 'w') as f:\n",
    "    for line in prepped_texts:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(prepped_file, 'w',encoding='utf-8') as f:\n",
    "    for line in docs1:\n",
    "        f.write(line+\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.train_unsupervised(prepped_file, model='cbow', wordNgrams=1,neg=10, ws=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.get_nearest_neighbors('typo'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_nearest_neighbors('argument')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_subwords('subwords')\n",
    "print(model.get_input_matrix().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.vocab import Vocab\n",
    "\n",
    "#im = model.get_input_matrix()\n",
    "vocab = Vocab()\n",
    "for i, key in enumerate(model.labels):\n",
    "    print([idx for j, idx in enumerate(model.get_subwords(str(key))[0])])\n",
    "    sws, idxs = model.get_subwords(str(key))\n",
    "    print(idxs[0])\n",
    "    #print(model.get_input_vector('the'))\n",
    "    pieces = [model.get_input_vector(idx) for j, idx in enumerate(idxs)]\n",
    "    #[print(k) for k in model.get_subwords(str(key))[1]]\n",
    "    pieces = np.sum(np.array(pieces), axis=0)/7\n",
    "    if i ==0:\n",
    "        print(model.get_subwords(str(key)))\n",
    "        print(pieces)\n",
    "        break\n",
    "    #print(pieces.shape)\n",
    "    vocab.set_vector(key, pieces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### print(vocab.get_vector('the'))\n",
    "print(model.get_output_matrix()[0])\n",
    "print(model.labels[0])\n",
    "print(model['the'])\n",
    "print(model.get_subwords('the'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.get_output_matrix()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.is_quantized())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(model.get_subwords.__func__))\n",
    "print(dir(model.get_subwords.__func__.__code__))\n",
    "print(model.get_subwords.__func__.__defaults__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labs = model.labels\n",
    "sw, idx =model.get_subwords(labs[0])\n",
    "subword_dict = {}\n",
    "i = 0\n",
    "for doc in docs:\n",
    "    #print(doc)\n",
    "    for lab in doc.split():\n",
    "        if i % 10000 == 0:\n",
    "            print(i, lab)\n",
    "        i+=1\n",
    "        #print(lab)\n",
    "        sws, idxs = model.get_subwords(lab)\n",
    "        #print(sws, idxs)\n",
    "        for sw, idx in zip(sws, idxs.tolist()):\n",
    "            subword_dict[idx]=sw\n",
    "print(len(subword_dict.keys()))\n",
    "#print(model.get_input_matrix().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(set(subword_dict.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {idx:sw for doc in docs for lab in doc.split() for sw, idx in zip(*model.get_subwords(lab))}\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(d.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.vocab import Vocab\n",
    "\n",
    "vocab = Vocab()\n",
    "for idx, sw in d.items():\n",
    "    vocab.set_vector(sw, model.get_input_vector(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_spot ='save_spot.bin'\n",
    "model.save_model(s_spot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "\n",
    "nlp = Language()\n",
    "\n",
    "with open('save_spot.bin', \"rb\") as file_:\n",
    "    header = file_.readline()\n",
    "    nr_row, nr_dim = header.split()\n",
    "    nlp.vocab.reset_vectors(width=int(nr_dim))\n",
    "    for line in file_:\n",
    "        line = line.rstrip().decode(\"utf8\")\n",
    "        pieces = line.rsplit(\" \", int(nr_dim))\n",
    "        word = pieces[0]\n",
    "        vector = numpy.asarray([float(v) for v in pieces[1:]], dtype=\"f\")\n",
    "        nlp.vocab.set_vector(word, vector)  # add the vectors to the vocab\n",
    "# test the vectors and similarity\n",
    "text = \"class colspan\"\n",
    "doc = nlp(text)\n",
    "print(text, doc[0].similarity(doc[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.labels)\n",
    "model.save_model(save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.load_model(save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
